{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89da8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: albumentations in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.0.7)\n",
      "Requirement already satisfied: numpy>=1.24.4 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from albumentations) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from albumentations) (1.13.1)\n",
      "Requirement already satisfied: PyYAML in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from albumentations) (6.0.2)\n",
      "Requirement already satisfied: pydantic>=2.9.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from albumentations) (2.10.6)\n",
      "Requirement already satisfied: albucore==0.0.24 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from albumentations) (0.0.24)\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from albumentations) (4.11.0.86)\n",
      "Requirement already satisfied: stringzilla>=3.10.4 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from albucore==0.0.24->albumentations) (3.12.5)\n",
      "Requirement already satisfied: simsimd>=5.9.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from albucore==0.0.24->albumentations) (6.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pydantic>=2.9.2->albumentations) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pydantic>=2.9.2->albumentations) (4.12.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3.12 install albumentations\n",
    "!pip3.12 install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "312f0559",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 1. Custom preprocessing classes\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "class HomomorphicFilter:\n",
    "    \"\"\"Enhances contrast by attenuating low-frequency and boosting high-frequency.\"\"\"\n",
    "    def __init__(self, a=0.5, b=1.5):\n",
    "        self.a, self.b = float(a), float(b)\n",
    "    def __butterworth_filter(self, shape, params):\n",
    "        P, Q = shape[0]//2, shape[1]//2\n",
    "        U, V = np.meshgrid(np.arange(shape[0]), np.arange(shape[1]), indexing='ij')\n",
    "        D = (U-P)**2 + (V-Q)**2\n",
    "        H = 1.0 / (1.0 + (D/(params[0]**2))**params[1])\n",
    "        return 1.0 - H\n",
    "    def __gaussian_filter(self, shape, params):\n",
    "        P, Q = shape[0]//2, shape[1]//2\n",
    "        U, V = np.meshgrid(np.arange(shape[0]), np.arange(shape[1]), indexing='ij')\n",
    "        D = (U-P)**2 + (V-Q)**2\n",
    "        H = np.exp(-D/(2*(params[0]**2)))\n",
    "        return 1.0 - H\n",
    "    def __apply_filter(self, I_fft, H):\n",
    "        Hs = np.fft.fftshift(H)\n",
    "        return (self.a + self.b*Hs) * I_fft\n",
    "    def filter(self, I, filter_params, mode='butterworth', H_ext=None):\n",
    "        if I.ndim != 2:\n",
    "            raise ValueError(\"Input must be single-channel\")\n",
    "        I_log = np.log1p(I.astype(float))\n",
    "        I_fft = np.fft.fft2(I_log)\n",
    "        if mode == 'butterworth':\n",
    "            H = self.__butterworth_filter(I_fft.shape, filter_params)\n",
    "        elif mode == 'gaussian':\n",
    "            H = self.__gaussian_filter(I_fft.shape, filter_params)\n",
    "        elif mode == 'external' and H_ext is not None:\n",
    "            H = H_ext\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown filter mode {mode}\")\n",
    "        I_filt = np.fft.ifft2(self.__apply_filter(I_fft, H))\n",
    "        I_out = np.exp(np.real(I_filt)) - 1\n",
    "        return np.uint8(np.clip(I_out, 0, 255))\n",
    "\n",
    "\n",
    "def relief_transform(img: np.ndarray, bias: int = 128) -> np.ndarray:\n",
    "    \"\"\"Diagonal-difference relief filter to accentuate gradients.\"\"\"\n",
    "    if img.ndim != 2:\n",
    "        raise ValueError(\"Expect grayscale\")\n",
    "    h, w = img.shape\n",
    "    out = np.zeros((h, w), dtype=np.int16)\n",
    "    out[1:-1,1:-1] = (\n",
    "        img[:-2,:-2].astype(int)\n",
    "        - img[2:,2:].astype(int)\n",
    "        + bias\n",
    "    )\n",
    "    out = np.clip(out, 0, 255).astype(np.uint8)\n",
    "    out[0,:], out[-1,:], out[:,0], out[:,-1] = img[0,:], img[-1,:], img[:,0], img[:,-1]\n",
    "    return out\n",
    "\n",
    "\n",
    "class HECRTransform(A.ImageOnlyTransform):\n",
    "    \"\"\"BGR → [gray | homo+CLAHE | relief+CLAHE] for weld-defect X-rays.\"\"\"\n",
    "    def __init__(self, clip_limit=5.0, bias=128, p=1.0):\n",
    "        super().__init__(p=p)\n",
    "        self.clip_limit = clip_limit\n",
    "        self.bias = bias\n",
    "        self.homo = HomomorphicFilter(a=0.75, b=1.25)\n",
    "    def apply(self, img, **kwargs):\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        eroded = cv2.erode(img, np.ones((3,3), np.uint8), iterations=1)\n",
    "        h = self.homo.filter(eroded[:,:,0], filter_params=[30,2])\n",
    "        clahe = cv2.createCLAHE(clipLimit=self.clip_limit)\n",
    "        ch1 = clahe.apply(h)\n",
    "        rel = relief_transform(gray, self.bias)\n",
    "        ch2 = clahe.apply(rel)\n",
    "        return np.stack([gray, ch1, ch2], axis=2)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 2. Configuration\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "WEIGHTS     = \"last-4.pt\"\n",
    "IMAGE_PATH  = \"0-340-ls-14-d01.png\"\n",
    "OUTPUT_DIR  = \"outputs\"\n",
    "IMG_SIZE    = 1024\n",
    "CONF_THRES  = 0.05\n",
    "TILE        = 1140    \n",
    "STRIDE      = int(TILE * 0.8)  # overlap stride      \n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 3. Prediction on tiled image\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "def predict_large_image():\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    # Load model\n",
    "    model = YOLO(WEIGHTS)\n",
    "    model.fuse()\n",
    "\n",
    "    # Read the large image\n",
    "    img = cv2.imread(IMAGE_PATH)\n",
    "    if img is None:\n",
    "        raise FileNotFoundError(f\"Image not found: {IMAGE_PATH}\")\n",
    "    h, w = img.shape[:2]\n",
    "\n",
    "    transformer = A.Compose([HECRTransform(p=1.0)])\n",
    "    all_preds = []\n",
    "    y0 = 0\n",
    "    y1 = y0 + TILE\n",
    "\n",
    "    # Slide window horizontally (height matches TILE exactly)\n",
    "    for x0 in range(0, w - TILE + 1, STRIDE):\n",
    "        x1 = x0 + TILE\n",
    "        crop = img[y0:y1, x0:x1]\n",
    "        proc = transformer(image=crop)[\"image\"]\n",
    "\n",
    "        # Inference on crop\n",
    "        results = model.predict(\n",
    "            source=proc,\n",
    "            imgsz=IMG_SIZE,\n",
    "            conf=CONF_THRES,\n",
    "            verbose=False\n",
    "        )\n",
    "        res = results[0]\n",
    "\n",
    "        # Extract detections\n",
    "        boxes = res.boxes.xyxy.cpu().numpy()    # (N,4)\n",
    "        confs = res.boxes.conf.cpu().numpy()    # (N,)\n",
    "        classes = res.boxes.cls.cpu().numpy().astype(int)  # (N,)\n",
    "\n",
    "        # Offset boxes back to full-image coords\n",
    "        for (xmin, ymin, xmax, ymax), conf, cls in zip(boxes, confs, classes):\n",
    "            all_preds.append([\n",
    "                xmin + x0,\n",
    "                ymin + y0,\n",
    "                xmax + x0,\n",
    "                ymax + y0,\n",
    "                conf,\n",
    "                cls\n",
    "            ])\n",
    "\n",
    "    # Save all predictions\n",
    "    pred_arr = np.array(all_preds)\n",
    "    out_file = os.path.join(OUTPUT_DIR, \"predictions.txt\")\n",
    "    np.savetxt(\n",
    "        out_file,\n",
    "        pred_arr,\n",
    "        fmt=['%.2f','%.2f','%.2f','%.2f','%.4f','%d'],\n",
    "        header='x1 y1 x2 y2 conf cls',\n",
    "        comments=''\n",
    "    )\n",
    "    print(f\"✅ Saved {len(all_preds)} predictions to {out_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24d87590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO11n-seg summary (fused): 113 layers, 2,837,103 parameters, 0 gradients, 10.2 GFLOPs\n",
      "✅ Saved 103 predictions to outputs/predictions.txt\n"
     ]
    }
   ],
   "source": [
    "predict_large_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ea03911",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_image_from_txt(image_path: str, preds_path: str, out_path: str):\n",
    "    \"\"\"\n",
    "    Reads a predictions.txt (with header 'x1 y1 x2 y2 conf cls'),\n",
    "    draws boxes & labels on the image, and saves to out_path.\n",
    "    \"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        raise FileNotFoundError(f\"Image not found: {image_path}\")\n",
    "\n",
    "    # Load predictions (skip header)\n",
    "    lines = open(preds_path).read().strip().splitlines()\n",
    "    if not lines:\n",
    "        print(\"No predictions to annotate.\")\n",
    "        return\n",
    "    if lines[0].startswith('x1'):\n",
    "        lines = lines[1:]\n",
    "\n",
    "    for ln in lines:\n",
    "        x1, y1, x2, y2, conf, cls = ln.split()\n",
    "        x1, y1, x2, y2 = map(float, (x1, y1, x2, y2))\n",
    "        conf = float(conf)\n",
    "        cls = int(cls)\n",
    "        # Draw rectangle\n",
    "        color = tuple(int(c) for c in np.random.randint(0, 255, (3,)))\n",
    "        cv2.rectangle(img,\n",
    "                      (int(x1), int(y1)),\n",
    "                      (int(x2), int(y2)),\n",
    "                      color=color,\n",
    "                      thickness=2)\n",
    "        label = f\"{cls}:{conf:.2f}\"\n",
    "        # Text background\n",
    "        (tw, th), _ = cv2.getTextSize(label,\n",
    "                                     cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                                     fontScale=0.5, thickness=1)\n",
    "        cv2.rectangle(img,\n",
    "                      (int(x1), int(y1)-th-4),\n",
    "                      (int(x1)+tw, int(y1)),\n",
    "                      color, -1)\n",
    "        cv2.putText(img, label,\n",
    "                    (int(x1), int(y1)-2),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5, (255,255,255), 1)\n",
    "\n",
    "    cv2.imwrite(out_path, img)\n",
    "    print(f\"✅ Saved annotated image to {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0864247f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved annotated image to outputs/annotated_large_image.png\n"
     ]
    }
   ],
   "source": [
    "annotated_path = os.path.join(OUTPUT_DIR, \"annotated_large_image.png\")\n",
    "annotate_image_from_txt(IMAGE_PATH, os.path.join(OUTPUT_DIR, \"predictions.txt\"), annotated_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
